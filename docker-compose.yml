services:
  # TEXT
  text-generation-webui:
    build: text/text-generation-webui
    ports:
      - "7860:7860"
      - "5000:5000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  ollama-backend:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ./text/ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  open-webui:
    image: ghcr.io/open-webui/open-webui
    ports:
      - "8080:8080"
    environment:
      - WEBUI_AUTH=False
      - OLLAMA_BASE_URL=http://ollama-backend:11434
    volumes:
      - ./text/open-webui:/app/backend/data
    depends_on:
      - ollama-backend


  # IMAGES
  stable-diffusion-webui:
    user: 1000:1000
    build: images/stable-diffusion-webui
    entrypoint: [ "python3", "webui.py", "--listen", "--port", "8080"]
    volumes:
      - "./images/stable-diffusion-webui/outputs:/app/outputs"
      - "./images/stable-diffusion-webui/models:/app/models"
      - "./images/stable-diffusion-webui/embeddings:/app/embeddings"
      - "./images/stable-diffusion-webui/extensions:/app/extensions"
    ports:
      - "8080:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  forge:
    user: 1000:1000
    build: images/forge
    entrypoint: ["python", "-u", "webui.py", "--listen", "--port", "8080", "--cuda-malloc"]
    volumes:
      - "./images/forge/outputs:/app/outputs"
      - "./images/forge/models:/app/models"
      - "./images/forge/embeddings:/app/embeddings"
      - "./images/forge/extensions:/app/extensions"
    ports:
      - "8080:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

  ace-step:
    build: music/ace-step
    entrypoint: ["python3", "./acestep/gui.py", "--server_name", "0.0.0.0", "--bf16", "true", "--port", "8080", "--torch_compile", "True"]
    ports:
      - "8080:8080"
    volumes:
      - "./music/ace-step/outputs:/app/outputs"
      - "./music/ace-step/checkpoints:/home/user/.cache/ace-step/checkpoints"
    environment:
      - ACE_OUTPUT_DIR=/app/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]